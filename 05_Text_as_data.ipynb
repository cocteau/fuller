{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we start**. We need to install two more \"conda\" packages. These will be easy, don't worry! Before you start your notebook, enter these two commmands. We will see how to install packages from R shortly, but this particular little dance requires a little extra.\n",
    "\n",
    ">`conda install -y -c anaconda libxml2`\n",
    "<br>\n",
    "`conda install -y -c r r-xml2` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text\n",
    "----\n",
    "\n",
    "Documents are part of your everyday practice. Whether from a FOIA request or \"scraped\" from the web, you will inevitably have to spend time with a raft of text files. Sometimes, it's sufficient to simply read them and, well, take notes. Those notes, your observations about the people or places or events in the documents, can become the basis for further reporting. Depending on the number of documents and their content, you might want to be more systematic in your note taking. You might want to track specific aspects of your documents, perhaps looking for patterns among the people involved, or over time, or across cities. This suggests taking what is referred to as \"unstructured\" data and turning it into something \"structured.\" \n",
    "\n",
    "Contrast [the web page of Obama's commutations](https://www.justice.gov/pardon/obama-commutations) with the various data frames we've examined in R from the FDOC. This is a list of people granted pardons and commuations under [the clemency initiative](https://www.justice.gov/pardon/clemency-initiative). As a web page, the list of commutations are meant to be read. You can visually scan the page, or perhaps search the page for years (like 2008) or places (like Iowa). Our data frames, on the other hand, let us immediately tally up how many rows match a given pattern. We can perform computations on each column, examine subsets of the data, produce informative summary graphics, and fit models to cluster rows that have similar characteristics.\n",
    "\n",
    "So how do we take the free text of [the commutations web page](https://www.justice.gov/pardon/obama-commutations) and systematically fill in a more structured data set? To be concrete, let's consider making a data frame where each row is a commuted sentence. What variables do we want to record for each commutation and how do we identify the information on the web page?\n",
    "\n",
    "As an exmple, consider the \"District/Date\" entries. Here are a few.\n",
    "\n",
    ">District of Connecticut; April 21, 2009\n",
    "<br>District of South Carolina; February 21, 2008\n",
    "<br>Eastern District of Virginia; June 19, 2008\n",
    "<br>Western District of Michigan; December 18, 2006\n",
    "\n",
    "For each commutation, we might want to split the \"where\" and the \"when\" and then populate two columns in a data frame. Now look at a few entries in the \"Sentences\" rows.\n",
    "\n",
    ">360 months' imprisonment; eight years' supervised release\n",
    "<br>292 months' imprisonment; five years' supervised release\n",
    "<br>240 months' imprisonment; five years' supervised release\n",
    "<br>Life imprisonment; six yearsâ€™ supervised release\n",
    "\n",
    "Here things are a little different. Still we have a pattern that emerges in each description (jail time then supervised release), but the language might vary -- from months to years to a category like \"Life imprisonment.\" And with documents that evolve in time, there are inevitably shifts in formats and categories. The more time that passes from the first document in your collection to the last, the greater the chance that the structure of the documents changes. \n",
    "\n",
    "And that's the case here. In your browser, have a look at the \"source\" of the [commutations page](https://www.justice.gov/pardon/obama-commutations) (in Chrome, you select the \"View\" tab and then \"Developer\" and then \"Source\"). Search for \"&lt;table\". This will take you through the \"tables\" on the page. There's one table of pardons after an \"h2\" header with text \"January 17, 2017\" and another table after an \"h2\" header with text \"December 19, 2016\". \n",
    "\n",
    "Today we are going to look at text, or \"strings of character,\" and the ways we might extract data from them. We will consider simple matching to a more elaborate pattern language. Our text or document will be the [commutations web page](https://www.justice.gov/pardon/obama-commutations), which means we'll also have to talk about how we work with text coming from an HTML page. First, we'll review a popular library for working with strings of characters called \"stringr.\" Here is a [short vignette about stringr](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html).\n",
    "\n",
    "**Installing a package**\n",
    "\n",
    "We will start by installing the package. This brings new code to your computer from CRAN, the Comprehensive R Archive Network. You only have to do this once, or rather, any time improvements to the code are published -- improvements that you want to take advantage of. Packages are R's way of inviting community involvement. People from different universities, research groups, companies, the general public... contribute new code to extend the reach of R to new data types or to introduce new ways of analyzing or visualizing data.\n",
    "\n",
    "Installing a package is like installing an app in the sense that new software will be downloaded to your computer. As with an app, new versions appear as the authors continue to refine and update their software. The command old.packages() tells you which packages on your computer have newer versions on CRAN, and update.packages() can be used to install the updates. Note that you might not want to automatically update a package when a new version appears. Sometimes the changes can be significant and you might not be ready to learn something new. \n",
    "\n",
    "Let's install the package \"stringr\" (if you did this in class, there's no need to do it again). We noticed in class that some people had to restart their kernels (go to the \"Kernel\" tab in the notebook menu and select \"Restart\") after they installed the package. \n",
    "\n",
    "(Oh and **\"kernel\"** is a general term for a computing service. Here we are \"shift-enter\" sending commands to R that computes something for us and returns a result. We refer to this as an R kernel. Restarting it kills the program and starts it up again.)\n",
    "\n",
    "OK onto \"stringr\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('stringr', repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a package is installed, we can load it with the library() command. This lets us take all the great work offered by the package author and use it. While you only have to install  a package once, you have to call the library() command in every session you want to use its functionality. \n",
    "\n",
    "Here we load up \"stringr.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stringr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple string manipulations**\n",
    "\n",
    "As we have seen even with the FDOC data, character strings often have to be cleaned up in various ways. It could require extracting a substring, concatenating strings together, or trimming off meaningless characters. The \"stringr\" library is good for this, providing a single naming convention for all its functions. We take you through some of what you can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string of characters, surrounding them with double or single quotes\n",
    "# Here we store two strings giving one the name \"disdate\" and the other the name \"sent\"\n",
    "# (for district/date and sentence)\n",
    "\n",
    "disdate = \"District of South Carolina; March 29, 2004\"\n",
    "sent = \"Life imprisonment; 10 years' supervised release\"\n",
    "\n",
    "# Create a new string by concatenating several together. This is like paste() from our last drill.\n",
    "# Here we concatenate \"disdate\" and \"sent\" to make one big string. Call it \"several\" \n",
    "# and then print it out.\n",
    "\n",
    "several = str_c(disdate,sent)\n",
    "several"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the number of characters, or the \"length\" of each string\n",
    "str_length(several)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a slightly nicer function to create a substring. As with substr() from the last drill,\n",
    "# you can specify the start and end, but the end might be negative, meaning counting\n",
    "# backwards from the last charcter in the string.\n",
    "\n",
    "# The first 10 characters\n",
    "str_sub(disdate,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth character to the end\n",
    "str_sub(disdate,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth character from the right to the end\n",
    "str_sub(disdate,-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, character 12 up until the third from the right \n",
    "str_sub(disdate,12,-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate a string\n",
    "str_dup(\"Term\",10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove white space at the start or end of the string -- particularly useful for\n",
    "# processing data \"scraped\" from a web site as HTML ignores spaces\n",
    "str_trim(\" Term     \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regular expressions**\n",
    "\n",
    "The package \"stringr\" also includes some pretty elegant functions for defining and extracting patterns of characters in strings. They all use a mini-language called \"regular expressions\" to specify the patterns.\n",
    "\n",
    "If you look over the sentences given out on the [commutations web page](https://www.justice.gov/pardon/obama-commutations), you'll see that some include fines. Looking over a few of them (search for \"fine\" on the page), you see that they have a common form. They are a dollar sign \\$ followed by a series of numbers that might include a comma. \n",
    "\n",
    "Regular expressions are a way of specifying patterns in text. We have circulated a document that describes this mini language for patterns and will go over it in class. For the moment, it's enough to know that regular expressions have syntax to specify matching a dollar sign, then any digit from 0 to 9, or a comma, (the so called \"character class\" defined by [0-9,]) that occurs one or more times (that's a +). \n",
    "\n",
    "A regular expression is just a string that specifies a pattern like this. In the box below we create a vector of four sentences, two with fines and two without. We also specify the regular expression string that defines a fine as we did in the paragraph above. We store the pattern in the string variable \"fine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = c(\"262 months' imprisonment; seven years' supervised release; $500 fine\", \n",
    "          \"262 months' imprisonment; five years' supervised release\", \n",
    "          \"180 months' imprisonment; five years' supervised release\", \n",
    "          \"Life imprisonment; five years' supervised release; $250 fine\"\n",
    "         )\n",
    "\n",
    "fine = \"\\\\$[0-9,]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a pattern and vector of strings, we start by simply testing which strings contain the pattern. Think of this as a kind of search engine query. The function str_detect() returns a boolean vector of TRUEs and FALSEs, indicating whether or not a string contains a pattern. This will be handy in filter(), for example. The function str_subset() returns just the strings that have the pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_detect(sents,fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first sentence has a fine, the second and third do not, and the fourth does -- TRUE, FALSE, FALSE, TRUE. Here are just the first and fourth strings with the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_subset(sents,fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the first and fourth strings refer to a fine and are correctly detected (whew). We can also determine the character number in each string where the pattern first appears and where the pattern ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_locate(sents,fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, look at the first sentence. \n",
    "\n",
    ">262 months' imprisonment; seven years' supervised release; $500 fine\n",
    "\n",
    "Count from the first character of the string, the 2. Count over 60 characters. So the \"2\" is the first, the \"6\" is the second, the \"m\" in \"months'\" is the fifth and the dollar sign in \"\\$500\" is the 60th. The last zero in \"\\$500\" is the 63. Do the same for the last sentence and make sure that dollar sign in \"\\$250\" is the 52nd character in the string.\n",
    "\n",
    "Now, if we know where the pattern starts and stops, we can extract just that data with a substring command. There is a built-in function for this called str_extract()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_extract(sents,fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice you get an NA or missing value when the pattern does not exist in the string. Hence sentences two and three have NA values. \n",
    "\n",
    "Finally, we can replace the identified pattern for something else. Here we replace the fine with the word \"No\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_replace(sents,fine,\"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we change gears a little and you look over the [commutations web page](https://www.justice.gov/pardon/obama-commutations), you'll see that there are fines as well as forfeitures and other penalties that are specified in dollars. So our regular expression is too loose if we are just looking for fines. We might want to follow our dollar-number/comma pattern with the word \"fine\".\n",
    "\n",
    "Before we leave this, we will run through the basics of regular expressions. Each of the expressions in the accompanying PDF I've circulated can be used \"as is\" with the slight caveat that you have to treat backslashes gently. A backslash, as you will see, in regular expression land means to \"escape\" a special character, making it mean itself. So \\$ means the end of the line in a regular expression, and \\\\\\$ means dollars. Because R uses the backslash as an escape character to define \"character constants\" like a tab, \"\\t\", or a newline, \"\\n\" we have to force R to read the backslash as a backslash. That means you have to, well, escape it with a backslash -- making the backslash mean a backslash. Oy. Hence our double backslashes in our pattern for fines. \n",
    "\n",
    "This sounds bad but it's an easy rule to remember. In R, we just double any backslash.\n",
    "\n",
    "For practice, we are going to look at lines that come from a series of emails released by Jeb Bush while he was governor of Florida. The site was taken down sometime last year but [here is a capture from the Internet Archive](https://web.archive.org/web/20150324042711/http://jebbushemails.com/home). We are going to use sentences pulled from these emails as a test case for our work on regular expressions. First, download the data (from courseworks or from [this link](http://compute-cuj.org/jeb/jeb.txt) and put it in the same folder as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeb = readLines(\"jeb.txt\")\n",
    "head(jeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"Elian\"\n",
    "head(str_subset(jeb,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"^I hope\"\n",
    "head(str_subset(jeb,pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Scraping\" data from the web**\n",
    "\n",
    "[The commutations page from the Justice Department](https://www.justice.gov/pardon/obama-commutations) stores the data we are after in the form of an HTML table. We need to pull the data out and load it into R. For this, we need another package, \"rvest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('rvest', repos='http://cran.us.r-project.org')\n",
    "install.packages('selectr', repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load it up and kick the tires. We also load \"dplyr\" because eventually we're going to create a data frame! And, as you recall, all the \"dplyr\" functions take data frames as arguments and manipulate them in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rvest)\n",
    "library(selectr)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"rvest\" contains a number of functions to bring web pages into R, making them expressive. By that I mean, you have the capacity to search for various tags and identify content. Hadley Wickham has written  [a simple description of rvest](https://github.com/hadley/rvest).\n",
    "\n",
    "Now, to specify what you want to extract from a web page, the browser plugin SelectorGadget is extremely handy. It's use is documented here. Please look over the documentation and follow along. It will have you install SelectorGadget and show you how it works.\n",
    "\n",
    "> [https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)\n",
    "\n",
    "Why do we need something like SelectorGadget? Or why do I think it's awesome? Well, HTML, the HyperText Markup Language, is designed to format information as a document. So there are \"tags\" to define paragraphs and headings and lists. When we publish data like the commutations as a web page, in HTML, details about each commutation might be formatted as an element in a table or a list item, and perhaps be associated with a particular font choice or specifying the text as \"bold\". We make these decisions based on how we think about the web page and how it will look.\n",
    "\n",
    "On the flip side, when you are given information in HTML as a web page, then, the document tags and styling often become important tools to help us reverse engineer what was in the mind of the designer, helping us to identify  how the details of each commutation were formatted. \n",
    "\n",
    "So, SelectorGadget examines a web page and produces a \"CSS selector\" that identifies just the data you want. CSS selectors are [described here](https://gist.github.com/smutnyleszek/809a69dd05e1d5f12d01) and are tools for associating styling with tags. Do you want headings a certain color or rendered in a certain font? Selectors let you specify that. Our task is to reverse engineering this logic, using the tag names and their styling to pull the data hiding in a web page. To bone up on CSS selectors, you might also try [this amusing game](http://flukeout.github.io/).\n",
    "\n",
    "This act of pulling or \"scraping\" data from web pages can be hard. There are plenty of online tools that do it now, with varying degrees of success. To get us started, we'll pull just the dates of **Obama's pardon announcements.** (The page is smaller and a little easier to work with than the much larger commutations.) They are listed at the top of the [Obama pardons page](https://www.justice.gov/pardon/obama-pardons) and we can use SelectorGadget to find their \"CSS selector,\" the assignment of styling to tags, that gets us the dates we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = read_html(\"https://www.justice.gov/pardon/obama-pardons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function html_nodes() extracts various components from the web page. Here, for example, are all the \"h2\" headings and then all the anchor \"a\" tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_nodes(page,\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_nodes(page,\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to appreciate what we've done. The read_html() function has returned an object that we've called \"page\". It has brought the pardons web page into R in a way that we can program with. We have immediate hooks into all the elements on the page. We can extract any portion of the page we want by specifying a tag name and its styling, essentially, through a CSS selector.\n",
    "\n",
    "For the pardon dates, the nodes we are after are also anchor tags. But, they are anchor tags that are contained in a paragraph tag &lt;p&gt; having a \"class\" attribute of \"rtecenter\". Search the source of the pardons page for the string \"rtecenter\" and you'll see it appears in just one place on the page. \n",
    "\n",
    ">&lt;p class=\"rtecenter\"&gt;&lt;a href=\"#mayone\"&gt;May 20, 2004&lt;/a&gt; | &lt;a href=\"#decone\"&gt;December 21, 2006&lt;/a&gt; | &lt;a href=\"#julyone\"&gt;July 2, 2007&lt;/a&gt; | &lt;a href=\"#deconex\"&gt;December 10, 2007&lt;/a&gt; | &lt;a href=\"#marone\"&gt;March 24, 2008&lt;/a&gt; | &lt;a href=\"#novone\"&gt;November 24, 2008&lt;/a&gt; |&lt;br /&gt;&lt;a href=\"#dectwo\"&gt;December 23, 2008&lt;/a&gt; | &lt;a href=\"#janone\"&gt;January 19, 2009&lt;/a&gt;&lt;/p&gt;\n",
    "\n",
    "OK that's a little ugly, but you see the &lt;p&gt; tag and then a series of &lt;a&gt; tags that contain our dates. So, we can use a CSS selector to specify this. We want someting of class \"rtecenter\" and then the anchor tags it contains. Thankfully,  SelectorGadget can get us the required selector directly by simply highlighting the data we want. SelectorGadget will return \".rtecenter a\". Again, you can skim the resources above (2 minutes of work) to see what this means. \n",
    "\n",
    "For now, take the selector string and extract what we need from the \"page\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = html_nodes(page,\".rtecenter a\")\n",
    "dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the text contained in each of the anchors &lt;a&gt; with a function html_text()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text(dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or get the href attributes using html_attr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_attr(dts,\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. From the web page of Obama's pardons (https://www.justice.gov/pardon/obama-pardons), use SelectorGadget to identify the CSS selector for the &lt;a&gt; anchor tags for the \"Clemency Recipients\" under the presidents going back to George Bush Sr. Then use read_html() and html_nodes() to extract these links. There should be nine of them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding structure**\n",
    "\n",
    "As we saw in the last drill, strings of dates are not as useful as actual date objects. Here we create date objects from a variable \"txtdts\" that holds the text of the pardon dates for Obama. Then we look at the time between the commuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lubridate)\n",
    "\n",
    "txtdts = html_text(dts)\n",
    "objdts = mdy(txtdts)\n",
    "objdts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time between most recent and previous pardon\n",
    "objdts[1]-objdts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time between previous pardon and the set before that\n",
    "objdts[2]-objdts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to Obama's commutations.** Let's return to [Obama's commutations page](https://www.justice.gov/pardon/obama-commutations), as we observed above, the details of the commutations themselves are stored in tables. We can select all the tables using the CSS selector that specifies just \"table\" tags. Here we are looking for each &lt;table&gt; and don't care about the styling or class or any of that. So the selector is just the string \"table\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = read_html(\"https://www.justice.gov/pardon/obama-commutations\")\n",
    "\n",
    "tbs = html_nodes(page,\"table\")\n",
    "tbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the various tables using another data type in R called a \"list\". A list is just a bag of data, accessed sequentially - there is a first item, a second and so on. It can contain any kind of data you like. It is like a vector except that vectors can contain only one kind of data. Vectors can be all numbers or all strings or all TRUE/FALSE's. A list can store data of any kind. Have a look. \n",
    "\n",
    "Here we make a list with four items. A vector of numbers, a string, a floating point number and a vector of TRUE/FALSE's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(c(1,2,3), \"hi\", 3.56, c(TRUE,FALSE,TRUE))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the purpose of this diversion is to show you how to access data. A vector accesses data with square brackets [ ]. A list uses double square brackets [[ ]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to drive this home, the fourth object in the list \"x\" is a vector of TRUE and FALSE values. As a vector, it can be indexed with single square brackets [ ]. So, if we want the second element of this vector, which is the fourth element of the list \"x\", we would do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[4]][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with a list or a list-like object, the double brackets get us to individual data items. \n",
    "\n",
    "Back to our tables. We also get at individual tables in \"tbs\" using  square brackets. So tbs[[1]] gives us the first table in HTML format, and tbs[[2]] gives us the second. \n",
    "\n",
    "The function html_table() takes a single table and \"parses\" it into a data frame. This should look familiar! We do the first and tenth HTML tables on the commutation web page below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = html_table(tbs[[1]])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10 = html_table(tbs[[10]])\n",
    "t10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! Something familiar! We can use the dim() function to tell us how many rows are in each data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this does not mean 1660 or 210 commutations. This means 1660 different cells (marked off as yellow or white rows in the commutations web page) in the first table. See that the first commutation in the tenth table takes up the first five rows of \"t10\".\n",
    "\n",
    "Also notice that because the HTML page didn't include table headers, the data frames \"t1\" and \"t10\" have default column names \"X1\" and \"X2\". We can give them more descriptive names using the names() function. It lets us both extract the names as a vector as well as assign new names to the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(t1) = c(\"item\",\"description\")\n",
    "head(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can start to work with the data a little. Here are all the sentences from the first table. We can use \"dplyr\" to select just the rows there the \"item\" entry is \"Sentence:\". Notice how we are still using text that is formatted to be read, as a document. It's \"Sentence:\" (the word sentence and then a colon) and not just \"Sentence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = filter(t1,item==\"Sentence:\")\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling the two parts of the lesson together, we can skim our data set for fines. Ah but notice that the pattern we specified is a little too loose. We also get \"$3,634.75 restitution\". We need to decide if we want to keep that field or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine = \"\\\\$[0-9,]+\"\n",
    "\n",
    "str_subset(sents$description,fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create a new data frame called dd (for district/date) that contains just the rows of \"t1\" that refer to the district and date of conviction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The bigger picture**\n",
    "\n",
    "Now, we are going to iterate (loop) over all the tables in \"tbs\" that we extracted from the commutations page and create one large data frame. We will start with \"t1\" and then loop over the second, third, fourth and so on tables. We will let \"i\" hold the table number and then let \"i\" take on the value 2, 3, 4... all the way up to the length(tbs). \n",
    "\n",
    "In the code below, we do just what we did to make \"t1\" with the exception of a subsetting step. We take \"tt\" below and consider just the first two columns. We do this because the 9th table on the page turns out to have 3 columns and not 2. Look for the name \"Saboonchi\" in the source of the commutations web page and you'll see 3 &lt;td&gt; tags in one row. Again, my friends, data cleaning.\n",
    "\n",
    "For each table, we then rename the columns as we did for \"t1\" and rbind() them to a growing data frame called \"commutations\". The rbind() function takes the first data frame argument and adds the second to the end of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with t1 and then loop through the other tables, adding to\n",
    "# the big \"commutations\" data set\n",
    "\n",
    "commutations = t1\n",
    "\n",
    "for(i in 2:length(tbs)){\n",
    "    \n",
    "    tt = html_table(tbs[[i]])\n",
    "    \n",
    "    # Table 9 has 3 columns by mistake - the third is unnecessary \n",
    "    # so we take just the first two in all cases\n",
    "    \n",
    "    tt = tt[,1:2]\n",
    "    \n",
    "    # Rename the columns from X1 and X2\n",
    "    \n",
    "    names(tt) = c(\"item\",\"description\")\n",
    "    \n",
    "    # Add the current table to the end of \"commutations\" by row-binding them\n",
    "    \n",
    "    commutations = rbind(commutations,tt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(commutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(commutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the values in the \"item\" column. It has things like \"District/Date:\" and \"Sentence:\" and, it turns out, some other things. We also see that we'll have to be careful as \"Terms of Grant:\" and \"Terms of gran:\", say, are both entries. This is what we call data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(commutations$item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we keep just district and date. We store it in \"dd\" which we see has 1713 entries. We also see that some tables don't have District/Date -- how many are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = filter(commutations,item==\"District/Date:\")\n",
    "dim(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use mutate() to add a column to dd. It will be formed by extracting the date portion of the \"description\"'s. We pull the date part by looking for everything after a semi-colon. Looking at the dates above, this seems like a good guess. We then pass that portion of the description onto mdy() to turn it into a date object.\n",
    "\n",
    "Oh and in this case the regular expression specifies a semicolon followed by a space and then any character. The period \".\" is a wild card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = mutate(dd,dates=mdy(str_extract(description,\"; .+\")))\n",
    "head(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dates that failed to parse. They are returned as NA's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(dd,is.na(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the date might involve multiple dates! We'll need to clean these up manually. For the moment, let's make a histogram of the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(dd$dates,breaks=50,freq=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Read the conditions of the [Clemency Initiative](https://www.justice.gov/pardon/clemency-initiative) and tell me if anything seems out of sorts with these dates. Then do a little digging to see what's going on. As a hint, here is some code to look at all the commutations happening with sentencing dates that are in 2014 or later. Change that as you see fit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(dd,year(dates)>=2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our next lesson, we will turn the commutations data frame into one that has one row per inmate, not one row per commutation detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
