{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Narrative structure of models, part 2\n",
    "------------\n",
    "Before we start, let's take a moment to talk about 3 articles that make use of computation in an advanced way and the different techniques used to tell a story from data and models.\n",
    "\n",
    ">[Redit and Trump](https://fivethirtyeight.com/features/dissecting-trumps-most-rabid-online-following/)\n",
    "<br><br>\n",
    "[Surgeon's scorecard](https://projects.propublica.org/surgeons/)\n",
    "<br><br>\n",
    "[Decision Tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n",
    "\n",
    "**Regression**\n",
    "\n",
    "Before we leave regression (or generalize it), I want to talk a little about the classical statistical line of story telling -- statistical significance. As we have mentioned, storytelling in modern regression focuses on the coefficients. How big are they? What are their signs? The answers to these questions translate directly the way the world works. \n",
    "\n",
    "Of course, you never interpret statistical measures without some notion of uncertainty. In particular, the narrative is not just one about coefficient size and direction but also statistical significance. Your introductory class was probably needlessly vague about this idea. We are going to talk about a procedure called the bootstrap. It swaps mathematical or analytical results for computational effort. We'll use Galton's data as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "galton = read.csv(\"http://compute-cuj.org/galton.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = lm(child~midparent,data=galton)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(coefficients(fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap is a *resampling technique* that works by directly estimating the sampling distribution of the quantity you're interested in. It depends on repeating the randomization technique used to generate the data. In short, Galton selected children at random from the population around him. We can't quite repeat his experiment (given that over 100 years have passed) but we can take the 928 samples he took and uses those as a kind of approximation to the population he sampled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boot = rep(0,5000)\n",
    "\n",
    "for(i in 1:5000){\n",
    "    \n",
    "    which = sample(1:nrow(galton),nrow(galton),replace=TRUE)\n",
    "    bootsamp = galton[which,]\n",
    "    \n",
    "    bootfit = lm(child~midparent,data=bootsamp)\n",
    "    boot[i] = coefficients(bootfit)[2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sd(boot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(quantile(boot,c(0.025,0.975)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap is a general procedure. You can use it to come up with the classic confidence intervals, say for the mean of some population quantity estimated from a random sample of the population. As an example, let's consider the average children's heights in Galton's sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boot = rep(0,3000)\n",
    "\n",
    "for(i in 1:3000){\n",
    "    \n",
    "    which = sample(1:nrow(galton),nrow(galton),replace=TRUE)\n",
    "    \n",
    "    bootsamp = galton[which,]\n",
    "    boot[i] = mean(bootsamp$child)\n",
    "}\n",
    "\n",
    "print(quantile(boot,c(0.025,0.975)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "options(repr.matrix.max.cols=50, repr.matrix.max.rows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning**\n",
    "\n",
    "We are going to consider a general purpose learning machine. It was developed in large part by Leo Breiman -- he's written about the \"two cultures\" in modeling. A **decision tree** is statistical model that is at one time a useful visualization as well as a prediction engine.  \n",
    "\n",
    "The data comes from the election in 2008, and specifically during the primaries. It was developed by Amanda Cox, an outstanding visual journalist at The New York Times.\n",
    "\n",
    "![tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n",
    "\n",
    "Have a look. The tree is like a game of 20 questions. If you want to know how a county will vote, answer a few questions. What is the racial makeup? What about the HS graduation rate? Where in the country is the county? \n",
    "\n",
    "The questions are in the form of a tree -- as we have seen before, computer scientists grow their trees upside down. As you answer questions, you eventually end up at a leaf and a decision is made. Note that it's not perfect -- there is some uncertainty in the decision. \n",
    "\n",
    "To show that this isn't a one-off, ProPublica had a lovely project called the [Message Machine](https://projects.propublica.org/emails/) where they looked email messages sent out by [political campaigns](https://www.propublica.org/special/message-machine-you-probably-dont-know-janet) and [reverse engineered the logic that generated them.](https://www.propublica.org/article/message-machine-starts-providing-answers)\n",
    "\n",
    "\n",
    "Let's load up the data and see how such a thing comes into existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = read.csv(\"http://compute-cuj.org/primary.csv\",as.is=TRUE)\n",
    "\n",
    "head(primary)\n",
    "dim(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names(primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point this graphic was published Montana, Oregon, South Dakota, Indiana, Kentucky, West Virginia, Pennsylvania, and South Carolina had yet to have their primaries. In all, there should be 2,261 counties for analysis. Here's where NY state had landed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select(filter(primary,state_postal==\"NY\"),winner,county_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the various counties across the country voted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, given all these potential predictor variables, which ones \"explain\" county-level voting patterns? Suppose, for example, we start simply, and consider whether or not a majority of the county voted for Bush in 2004. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner,primary$pres04winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "290/(290+163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1047/(1047+740)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, Obama won about 64% of those counties not voting for Bush in 04, while Clinton won about 59% of those counties that did vote for Bush in 04. Now, consider the **simple prediction rule**: If a county voted for Bush in 04, we’ll say that they will vote for Clinton in the primary, while if a county was mostly in favor of Kerry in 04, we’ll assign the win to Obama. \n",
    "\n",
    "Of course this rule isn’t perfect; by applying it, we would make 163 + 740 =\n",
    "903 mistakes (out of 2,261 counties, or about 40% error); we refer to these mistakes as having been \"misclassified\" by our simple rule. So the question becomes, can we do any better? What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(163 + 740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(163 + 740)/2261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be better indicators of Obama’s success besides the vote in 2004 -- but how do we find them? The decision tree encodes a large search across the complete data set for predictors that are informative. \n",
    "\n",
    "Consider the top of the tree, \"the root\". Decision trees work by repeatedly splitting the data into two parts; the root or first \"split\" is the single division of the data into two pieces that produces the lowest misclassification error (well, it's a little more complicated, but this will do).\n",
    "\n",
    "To investigate this a little further, let’s consider the predictor that represents the\n",
    "percentage of a county that is African American; we now choose a breakpoint that divides the data into two pieces (those counties with a greater percentage of African Americans and those with a smaller percentage). We then form a table (as we did for counties that went for Bush or Kerry in 2004) and count the misclassification rate.\n",
    "\n",
    "Suppose we take 10% as the cutoff. We get the following table. How many errors do we make? What is the misclassification rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner,primary$black06pct > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Error rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20% figure at the root of the tree was obtained by finding the magic point that minimizes the misclassification errors. In fact, the search was conducted over all the variables in the data set and all the possible splits; and this choice produced the smallest error.\n",
    "\n",
    "Once this node has been chosen, we work our way down the tree, conducting the same search but on the specified subsets of the data, at each step attempting to minimize our errors. \n",
    "\n",
    "For quantitative variables, we have a choice of split points, and so the algorithm goes through a search. Here we vary a cutoff for the percentage of African Americans living in a county from essentially 0 to 0.5. For each choice of cutoff, we look at the associated error. \n",
    "\n",
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = (1:500)/1000\n",
    "\n",
    "n = length(fractions)\n",
    "error = rep(0,n)\n",
    "\n",
    "for(i in 1:n){\n",
    "    \n",
    "    # right branch\n",
    "    tmp = select(filter(primary,black06pct > fractions[i]),winner)\n",
    "    error.right = min(table(tmp))\n",
    "    \n",
    "    # left branch\n",
    "    tmp = select(filter(primary,black06pct <= fractions[i]),winner)\n",
    "    error.left = min(table(tmp))\n",
    "    \n",
    "    error[i] = error.left+error.right\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "\n",
    "plot(fractions,error,xlab=\"percentage black\",ylab=\"misclassification error\",type=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range(primary$Bush04,na.rm=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = (100:900)/1000\n",
    "\n",
    "n = length(fractions)\n",
    "error = rep(0,n)\n",
    "\n",
    "for(i in 1:n){\n",
    "    \n",
    "    # right branch\n",
    "    tmp = select(filter(primary,Bush04 > fractions[i]),winner)\n",
    "    error.right = min(table(tmp))\n",
    "    \n",
    "    # left branch\n",
    "    tmp = select(filter(primary,Bush04 <= fractions[i]),winner)\n",
    "    error.left = min(table(tmp))\n",
    "    \n",
    "    error[i] = error.left+error.right\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(fractions,error,xlab=\"Bush margin\",ylab=\"misclassification error\",type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what the decision tree process does. It's a greedy algorithm, meaning at each step, it is looking for the best way to divide the data into two pieces. There are things we can do to speed it up computationally, and there are issues with really trying to minimize misclassification rates, but this is essentially the approach.\n",
    "\n",
    "Stop and think what this simple process has produced for us; we have a very intuitive structure (something akin to the game 20 questions) that makes evident \"important\" variables that help \"explain\" voting patterns. This kind of tool lives somewhere between data analysis and modeling; it is technically a model all by itself (making predictions) but is often used as a way to identify important predictors for another stage of model.\n",
    "\n",
    "This decision tree is part of a large class of methods called CART for Classification and Regression Trees and was developed in the 1980s as part of a move to deal with bigger and meaner data sets.\n",
    "\n",
    "Here is the implementation in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "            growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,data=primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(xpd=TRUE)\n",
    "\n",
    "plot(fit)\n",
    "text(fit,use.n=TRUE,cex=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s think a bit more about the tree-growing process; with each split, we cut\n",
    "our data at the node into two pieces so that the “sample size” at each of the child nodes is lower than its parents. We then represent the data in the leaves with a simple model; for our 0/1 data (Obama or Clinton), we classify leaves according to majority vote.\n",
    "\n",
    "In principle we can grow trees until there’s a single entry in each node -- What\n",
    "might be the problem with that? How do we decide to stop splitting? When we\n",
    "run out of data? \n",
    "\n",
    "To grow a bigger tree, we can reduce \"cp\", the complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted by our algorithm, so making it smaller will produce a larger tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "            growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,data=primary,cp=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the R displays above, the heights of the branches correspond to the error in represented by the model. So, the at the root, we are dealing with all the counties -- 1210 went for Clinton\n",
    "and 1031 for Obama -- therefore since Clinton won more counties, we would\n",
    "predict all future counties for Clinton, making 1030 mistakes.\n",
    "\n",
    "The first split on the percentage of the county that is\n",
    "African American brought us down to 700 errors, a big drop; the next division\n",
    "based on education is another big drop, about half the size. As we continue to refine the tree, however, the improvements diminish.\n",
    "\n",
    "In the mid-1980s a fair bit of theoretical and methodological work was devoted\n",
    "to understanding the behavior of this kind of algorithm; we are using it as a bit\n",
    "of data analysis (how does a “response” relate to the potential “explanatory”\n",
    "variables?) but it can also be used as a tool for making predictions.\n",
    "\n",
    "The R command rpart (for recursive partitioning) employs a “pruning”\n",
    "algorithm that divides the original data into several parts; iteratively leaving one\n",
    "part out, building a tree and evaluating it on the part that was left out -- this is\n",
    "called cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotcp(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printcp(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(prune(fit,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lovely graphical introduciton to decision trees is [given here.](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "**Logistic regression**\n",
    "\n",
    "Decision trees are a relatively late addition to the statistian's toolbox. The narrative structure of a table as we have seen is also possible when our response is not normally distributed. So-called logistic regression uses \"binomial\" responses as the model. Think of a coin toss where the probability of turning up heads depends on a set of variables. So a county's primary is decided by a coin toss, the properties of that coin depending on demographics and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(factor(winner)~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "          growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,\n",
    "          data=primary,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = mutate(primary,biwinner = winner==\"obama\")\n",
    "head(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\")\n",
    "abline(lm(biwinner~Bush04,data=primary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~black06pct,data=primary,pch=\"|\")\n",
    "abline(lm(biwinner~black06pct,data=primary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~black06pct,data=primary,pch=\"|\",col=ifelse(biwinner,\"cyan\",\"magenta\"))\n",
    "abline(lm(biwinner~black06pct,data=primary))\n",
    "\n",
    "# ignore as slightly tedious \"behind the scenes\"\n",
    "points(c(0.1,0.3,0.5,0.7,0.9),sapply(split(primary$biwinner,cut(primary$black06pct,seq(0,1,len=6))),mean,na.rm=TRUE),pch=19)\n",
    "abline(v=c(0,0.2,0.4,0.6,0.8,1.0),lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\",col=ifelse(biwinner,\"cyan\",\"magenta\"))\n",
    "abline(lm(biwinner~Bush04,data=primary))\n",
    "\n",
    "# ignore as slightly tedious \"behind the scenes\"\n",
    "points(c(0.1,0.3,0.5,0.7,0.9),sapply(split(primary$biwinner,cut(primary$Bush04,seq(0,1,len=6))),mean,na.rm=TRUE),pch=19)\n",
    "abline(v=c(0,0.2,0.4,0.6,0.8,1.0),lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(black06pct~pct_hs_grad,data=primary,col=ifelse(biwinner,\"cyan\",\"magenta\"),pch=ifelse(biwinner,\"1\",\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(factor(winner)~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "          growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,\n",
    "          data=primary,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More recent data**\n",
    "\n",
    "Here is a data set collected by Loren Collingwood, a political scientist. It accumulates many of the same county-level variables but is looking at the recent national election instead. Let's try out our Rpart skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "election = read.csv(\"http://www.collingwoodresearch.com/uploads/8/3/6/0/8360930/county_data.csv\",sep=\"\\t\")\n",
    "head(election)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a column indicating the winner for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "election = mutate(election,winner = ifelse(pct_clinton < pct_trump,\"Trump\",\"Clinton\"))\n",
    "head(election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~per_capita_income+pobama12cnty+percent_white,data=election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(xpd=TRUE)\n",
    "\n",
    "plot(fit)\n",
    "text(fit,use.n=TRUE,cex=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collingwood didn't have a dictionary for his data, but he was able to answer a few questions. Here are the columns I couldn't read easily and his explanations.\n",
    "\n",
    "totpop1014cnty what does 1014 refer to? 1014 IS SAMPLED BETWEEN YEARS 2010-14 FOR ACES\n",
    "<br>\n",
    "ppiwht1014cnty   ppi? --I'M PRETTY SURE THIS IS PERCENT WHITE, THN PERCENT BLACK, ETC.\n",
    "<br>\n",
    "ppiblk1014cnty\n",
    "<br>\n",
    "ppihisp1014cnty\n",
    "<br>\n",
    "pimm1014cnty        mm? PERCENT IMMIGRANT\n",
    "<br>\n",
    "ppi0014cnty         ppi again and what is 0014? THE 0014 WOULD NOW BE A CHANGE MEASURE -- PERCENT CHANGE FROM 2000 TO 2014.\n",
    "<br>\n",
    "psocsec0014cnty\n",
    "<br>\n",
    "ownthirty0014cnty   ownthirty?\n",
    "<br>\n",
    "ownfifty0014cnty\n",
    "<br>\n",
    "pmanufact1014cnty   ? employment in manufacturing? CORRECT -- THIS WAS PREDICTOR OF TRUMP VOTE\n",
    "<br>\n",
    "pct65over00cnty     this is in 2000? YES, FOR OLD PEOPLE\n",
    "<br>\n",
    "medoohvalue00cnty    ? MEDIAN HOME VALUE 2000\n",
    "<br>\n",
    "pctcomlong00cnty    comlong? I THINK THIS IS COMMUTE OVER 30 MINS -- BUT I DIDN'T USE THIS SO NOT 100% ON THAT.\n",
    "<br>\n",
    "pctdiffh95_00cnty   ? NOT SURE\n",
    "<br>\n",
    "pctfb00cnty         ? FOREIGN BORN\n",
    "<br>\n",
    "pcpi00cnty           pcpi? NOT SURE\n",
    "<br>\n",
    "Rpt.                ? NOT SURE\n",
    "<br>\n",
    "state.y STATE NAME/CODE. PROBABLY A state.x AS WELL? FUNCTION OF MERGING IN R\n",
    "<br>\n",
    "pct_change_R         ?\n",
    "<br>\n",
    "manu_loss            ? IS THIS A DUMMY? IF SO, DID THE COUNTY RECEIVE MANUFACTURING LOSS IF YES = 1, ELSE = 0\n",
    "<br>\n",
    "rural                ? RURAL COUNTY BASED OFF OFFICIAL CENSUS DESIGNATION (DUMMY)\n",
    "<br>\n",
    "rural_150            ? SIMILAR MEASURE, THIS IS THE ONE I THINK IS BASED OF 100 PER SQ/MILE. BUT WOULD HAVE TO CHECK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
