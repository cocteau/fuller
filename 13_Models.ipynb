{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cheat sheets**\n",
    "\n",
    "[RStudio's Base R cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)\n",
    "\n",
    "[Data viz cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)\n",
    "\n",
    "[Data import cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf)\n",
    "\n",
    "[Task-based cheat sheet](https://www.analyticsvidhya.com/blog/2015/10/cheatsheet-11-steps-data-exploration-with-codes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "options(repr.matrix.max.cols=50, repr.matrix.max.rows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning\n",
    "-------\n",
    "\n",
    "We are going to start today with a general purpose learning machine. It was developed in large part by Leo Breiman -- he's written about the \"two cultures\" in modeling. A **decision tree** is statistical model that is at one time a useful visualization as well as a prediction engine.  \n",
    "\n",
    "The data comes from the election in 2008, and specifically during the primaries. It was developed by Amanda Cox, an outstanding visual journalist at The New York Times.\n",
    "\n",
    "![tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n",
    "\n",
    "Have a look. The tree is like a game of 20 questions. If you want to know how a county will vote, answer a few questions. What is the racial makeup? What about the HS graduation rate? Where in the country is the county? \n",
    "\n",
    "The questions are in the form of a tree -- as we have seen before, computer scientists grow their trees upside down. As you answer questions, you eventually end up at a leaf and a decision is made. Note that it's not perfect -- there is some uncertainty in the decision. \n",
    "\n",
    "To show that this isn't a one-off, ProPublica had a lovely project called the [Message Machine](https://projects.propublica.org/emails/) where they looked email messages sent out by [political campaigns](https://www.propublica.org/special/message-machine-you-probably-dont-know-janet) and [reverse engineered the logic that generated them.](https://www.propublica.org/article/message-machine-starts-providing-answers)\n",
    "\n",
    "\n",
    "Let's load up the data and see how such a thing comes into existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = read.csv(\"primary.csv\",as.is=TRUE)\n",
    "\n",
    "head(primary)\n",
    "dim(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names(primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point this graphic was published Montana, Oregon, South Dakota, Indiana, Kentucky, West Virginia, Pennsylvania, and South Carolina had yet to have their primaries. In all, there should be 2,261 counties for analysis. Here's where NY state had landed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select(filter(primary,state_postal==\"NY\"),winner,county_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the various counties across the country voted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, given all these potential predictor variables, which ones \"explain\" county-level voting patterns? Suppose, for example, we start simply, and consider whether or not a majority of the county voted for Bush in 2004. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner,primary$pres04winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "290/(290+163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1047/(1047+740)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, Obama won about 64% of those counties not voting for Bush in 04, while Clinton won about 59% of those counties that did vote for Bush in 04. Now, consider the **simple prediction rule**: If a county voted for Bush in 04, we’ll say that they will vote for Clinton in the primary, while if a county was mostly in favor of Kerry in 04, we’ll assign the win to Obama. \n",
    "\n",
    "Of course this rule isn’t perfect; by applying it, we would make 163 + 740 =\n",
    "903 mistakes (out of 2,261 counties, or about 40% error); we refer to these mistakes as having been \"misclassified\" by our simple rule. So the question becomes, can we do any better? What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(163 + 740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(163 + 740)/2261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be better indicators of Obama’s success besides the vote in 2004 -- but how do we find them? The decision tree encodes a large search across the complete data set for predictors that are informative. \n",
    "\n",
    "Consider the top of the tree, \"the root\". Decision trees work by repeatedly splitting the data into two parts; the root or first \"split\" is the single division of the data into two pieces that produces the lowest misclassification error (well, it's a little more complicated, but this will do).\n",
    "\n",
    "To investigate this a little further, let’s consider the predictor that represents the\n",
    "percentage of a county that is African American; we now choose a breakpoint that divides the data into two pieces (those counties with a greater percentage of African Americans and those with a smaller percentage). We then form a table (as we did for counties that went for Bush or Kerry in 2004) and count the misclassification rate.\n",
    "\n",
    "Suppose we take 10% as the cutoff. We get the following table. How many errors do we make? What is the misclassification rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(primary$winner,primary$black06pct > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Error rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20% figure at the root of the tree was obtained by finding the magic point that minimizes the misclassification errors. In fact, the search was conducted over all the variables in the data set and all the possible splits; and this choice produced the smallest error.\n",
    "\n",
    "Once this node has been chosen, we work our way down the tree, conducting the same search but on the specified subsets of the data, at each step attempting to minimize our errors. \n",
    "\n",
    "For quantitative variables, we have a choice of split points, and so the algorithm goes through a search. Here we vary a cutoff for the percentage of African Americans living in a county from essentially 0 to 0.5. For each choice of cutoff, we look at the associated error. \n",
    "\n",
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = (1:500)/1000\n",
    "\n",
    "n = length(fractions)\n",
    "error = rep(0,n)\n",
    "\n",
    "for(i in 1:n){\n",
    "    \n",
    "    # right branch\n",
    "    tmp = select(filter(primary,black06pct > fractions[i]),winner)\n",
    "    error.right = min(table(tmp))\n",
    "    \n",
    "    # left branch\n",
    "    tmp = select(filter(primary,black06pct <= fractions[i]),winner)\n",
    "    error.left = min(table(tmp))\n",
    "    \n",
    "    error[i] = error.left+error.right\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "\n",
    "plot(fractions,error,xlab=\"percentage black\",ylab=\"misclassification error\",type=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range(primary$Bush04,na.rm=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = (100:900)/1000\n",
    "\n",
    "n = length(fractions)\n",
    "error = rep(0,n)\n",
    "\n",
    "for(i in 1:n){\n",
    "    \n",
    "    # right branch\n",
    "    tmp = select(filter(primary,Bush04 > fractions[i]),winner)\n",
    "    error.right = min(table(tmp))\n",
    "    \n",
    "    # left branch\n",
    "    tmp = select(filter(primary,Bush04 <= fractions[i]),winner)\n",
    "    error.left = min(table(tmp))\n",
    "    \n",
    "    error[i] = error.left+error.right\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(fractions,error,xlab=\"Bush margin\",ylab=\"misclassification error\",type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what the decision tree process does. It's a greedy algorithm, meaning at each step, it is looking for the best way to divide the data into two pieces. There are things we can do to speed it up computationally, and there are issues with really trying to minimize misclassification rates, but this is essentially the approach.\n",
    "\n",
    "Stop and think what this simple process has produced for us; we have a very intuitive structure (something akin to the game 20 questions) that makes evident \"important\" variables that help \"explain\" voting patterns. This kind of tool lives somewhere between data analysis and modeling; it is technically a model all by itself (making predictions) but is often used as a way to identify important predictors for another stage of model.\n",
    "\n",
    "This decision tree is part of a large class of methods called CART for Classification and Regression Trees and was developed in the 1980s as part of a move to deal with bigger and meaner data sets.\n",
    "\n",
    "Here is the implementation in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "            growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,data=primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(xpd=TRUE)\n",
    "\n",
    "plot(fit)\n",
    "text(fit,use.n=TRUE,cex=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s think a bit more about the tree-growing process; with each split, we cut\n",
    "our data at the node into two pieces so that the “sample size” at each of the child nodes is lower than its parents. We then represent the data in the leaves with a simple model; for our 0/1 data (Obama or Clinton), we classify leaves according to majority vote.\n",
    "\n",
    "In principle we can grow trees until there’s a single entry in each node -- What\n",
    "might be the problem with that? How do we decide to stop splitting? When we\n",
    "run out of data? \n",
    "\n",
    "To grow a bigger tree, we can reduce \"cp\", the complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted by our algorithm, so making it smaller will produce a larger tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "            growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,data=primary,cp=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the R displays above, the heights of the branches correspond to the error in represented by the model. So, the at the root, we are dealing with all the counties -- 1210 went for Clinton\n",
    "and 1031 for Obama -- therefore since Clinton won more counties, we would\n",
    "predict all future counties for Clinton, making 1030 mistakes.\n",
    "\n",
    "The first split on the percentage of the county that is\n",
    "African American brought us down to 700 errors, a big drop; the next division\n",
    "based on education is another big drop, about half the size. As we continue to refine the tree, however, the improvements diminish.\n",
    "\n",
    "In the mid-1980s a fair bit of theoretical and methodological work was devoted\n",
    "to understanding the behavior of this kind of algorithm; we are using it as a bit\n",
    "of data analysis (how does a “response” relate to the potential “explanatory”\n",
    "variables?) but it can also be used as a tool for making predictions.\n",
    "\n",
    "The R command rpart (for recursive partitioning) employs a “pruning”\n",
    "algorithm that divides the original data into several parts; iteratively leaving one\n",
    "part out, building a tree and evaluating it on the part that was left out -- this is\n",
    "called cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotcp(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printcp(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(prune(fit,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lovely graphical introduciton to decision trees is [given here.](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "**Logistic regression**\n",
    "\n",
    "Decision trees are a relatively late addition to the statistian's toolbox. The narrative structure of a table as we have seen is also possible when our response is not normally distributed. So-called logistic regression uses \"binomial\" responses as the model. Think of a coin toss where the probability of turning up heads depends on a set of variables. So a county's primary is decided by a coin toss, the properties of that coin depending on demographics and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(factor(winner)~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "          growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,\n",
    "          data=primary,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = mutate(primary,biwinner = winner==\"obama\")\n",
    "head(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\")\n",
    "abline(lm(biwinner~Bush04,data=primary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~black06pct,data=primary,pch=\"|\")\n",
    "abline(lm(biwinner~black06pct,data=primary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~black06pct,data=primary,pch=\"|\",col=ifelse(biwinner,\"cyan\",\"magenta\"))\n",
    "abline(lm(biwinner~black06pct,data=primary))\n",
    "\n",
    "# ignore as slightly tedious \"behind the scenes\"\n",
    "points(c(0.1,0.3,0.5,0.7,0.9),sapply(split(primary$biwinner,cut(primary$black06pct,seq(0,1,len=6))),mean,na.rm=TRUE),pch=19)\n",
    "abline(v=c(0,0.2,0.4,0.6,0.8,1.0),lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(biwinner~Bush04,data=primary,pch=\"|\",col=ifelse(biwinner,\"cyan\",\"magenta\"))\n",
    "abline(lm(biwinner~Bush04,data=primary))\n",
    "\n",
    "# ignore as slightly tedious \"behind the scenes\"\n",
    "points(c(0.1,0.3,0.5,0.7,0.9),sapply(split(primary$biwinner,cut(primary$Bush04,seq(0,1,len=6))),mean,na.rm=TRUE),pch=19)\n",
    "abline(v=c(0,0.2,0.4,0.6,0.8,1.0),lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(black06pct~pct_hs_grad,data=primary,col=ifelse(biwinner,\"cyan\",\"magenta\"),pch=ifelse(biwinner,\"1\",\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(factor(winner)~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "          growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,\n",
    "          data=primary,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More recent data**\n",
    "\n",
    "Here is a data set collected by Loren Collingwood, a political scientist. It accumulates many of the same county-level variables but is looking at the recent national election instead. Let's try out our Rpart skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "election = read.csv(\"http://www.collingwoodresearch.com/uploads/8/3/6/0/8360930/county_data.csv\",sep=\"\\t\")\n",
    "head(election)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a column indicating the winner for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "election = mutate(election,winner = ifelse(pct_clinton < pct_trump,\"Trump\",\"Clinton\"))\n",
    "head(election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rpart(winner~per_capita_income+pobama12cnty+percent_white,data=election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(xpd=TRUE)\n",
    "\n",
    "plot(fit)\n",
    "text(fit,use.n=TRUE,cex=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collingwood didn't have a dictionary for his data, but he was able to answer a few questions. Here are the columns I couldn't read easily and his explanations.\n",
    "\n",
    "totpop1014cnty what does 1014 refer to? 1014 IS SAMPLED BETWEEN YEARS 2010-14 FOR ACES\n",
    "<br>\n",
    "ppiwht1014cnty   ppi? --I'M PRETTY SURE THIS IS PERCENT WHITE, THN PERCENT BLACK, ETC.\n",
    "<br>\n",
    "ppiblk1014cnty\n",
    "<br>\n",
    "ppihisp1014cnty\n",
    "<br>\n",
    "pimm1014cnty        mm? PERCENT IMMIGRANT\n",
    "<br>\n",
    "ppi0014cnty         ppi again and what is 0014? THE 0014 WOULD NOW BE A CHANGE MEASURE -- PERCENT CHANGE FROM 2000 TO 2014.\n",
    "<br>\n",
    "psocsec0014cnty\n",
    "<br>\n",
    "ownthirty0014cnty   ownthirty?\n",
    "<br>\n",
    "ownfifty0014cnty\n",
    "<br>\n",
    "pmanufact1014cnty   ? employment in manufacturing? CORRECT -- THIS WAS PREDICTOR OF TRUMP VOTE\n",
    "<br>\n",
    "pct65over00cnty     this is in 2000? YES, FOR OLD PEOPLE\n",
    "<br>\n",
    "medoohvalue00cnty    ? MEDIAN HOME VALUE 2000\n",
    "<br>\n",
    "pctcomlong00cnty    comlong? I THINK THIS IS COMMUTE OVER 30 MINS -- BUT I DIDN'T USE THIS SO NOT 100% ON THAT.\n",
    "<br>\n",
    "pctdiffh95_00cnty   ? NOT SURE\n",
    "<br>\n",
    "pctfb00cnty         ? FOREIGN BORN\n",
    "<br>\n",
    "pcpi00cnty           pcpi? NOT SURE\n",
    "<br>\n",
    "Rpt.                ? NOT SURE\n",
    "<br>\n",
    "state.y STATE NAME/CODE. PROBABLY A state.x AS WELL? FUNCTION OF MERGING IN R\n",
    "<br>\n",
    "pct_change_R         ?\n",
    "<br>\n",
    "manu_loss            ? IS THIS A DUMMY? IF SO, DID THE COUNTY RECEIVE MANUFACTURING LOSS IF YES = 1, ELSE = 0\n",
    "<br>\n",
    "rural                ? RURAL COUNTY BASED OFF OFFICIAL CENSUS DESIGNATION (DUMMY)\n",
    "<br>\n",
    "rural_150            ? SIMILAR MEASURE, THIS IS THE ONE I THINK IS BASED OF 100 PER SQ/MILE. BUT WOULD HAVE TO CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.matrix.max.cols=100,repr.matrix.max.rows=300)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit more on learning**\n",
    "\n",
    "Decision trees are a relatively late addition to the statistician's toolbox. It can be applied to regression as well as classification -- in that case, our observations at the leaves are just an average of the responses not a vote. As a learning algorithm, it has a lot to recommend it. It can adapt to a great deal of complexity and it is easy to explain. \n",
    "\n",
    "One downside comes from its \"greediness\" -- at each step in the process it picks the best division of the data into two pieces and small changes in the data can make big changes to the tree. There are ways around that problem and you will find people doing things like averaging several trees together, built from perturbed (um, bootstrapped!) data. You will find words like \"forests\" attached to this combination of trees.\n",
    "\n",
    "Journalistically, you can find trees at work in a variety of stories. The [Message Machine](https://www.propublica.org/article/reverse-engineering-obamas-message-machine) from ProPublica is a good example. Here they take emails sent by political campaigns and \"reverse engineer\" them to uncover which demographic variables determine the emails you receive. While there are some techincal terms we haven't talked a out in their [\"nerd box\"](https://www.propublica.org/nerds/item/how-propublicas-message-machine-reverse-engineers-political-microtargeting), but at the core you'll find trees.\n",
    "\n",
    "Peter Aldhous' discussion of machine learning highlights this point. He wanted the \"algorithm to discover\" what variables were important in determing whether a plane was in a surveillance pattern or not. That led him to create variables associated with each flight (time spent turning left, etc.) and then create a data set, one row per flight. Some he knew were surveillance and he tagged them as such. Others were not, and he tagged them as such. The tagged flights, together with their variables were then fed to an algorithm to learn what was important.\n",
    "\n",
    "To connect to our crime case, Azavea (HunchLab) has been contracted by New York City to create a predictive tree-based model. As [Politico put it](http://www.politico.com/states/new-york/city-hall/story/2015/07/nypd-testing-crime-forecast-software-090820)\n",
    "\n",
    ">The New York Police Department is embarking on a new experiment with “predictive policing” software that uses neighborhood-specific data like weather, time of year, school calendars and past criminal activity to create statistical models that forecast where and when certain crimes are likely to occur.\n",
    "\n",
    "If you look at their material, you'll see that they use a \"forest\" of trees, but again, the underlying technology is a tree. My point? That tools from machine learning can help you both with your reporting (Larson, Aldhous) and can be the subject of your story (predictive policing). \n",
    "\n",
    "**Logistic regression, just to put it close to the next topic**\n",
    "\n",
    "Regression is an extremely popular tool as well. It can be adapted to classification problems, the most popular version of which is \"logistic regression.\" That's a mouth full, but it's worth talking about from a methodological perspective. We can make comparisons with it as well as ordinary regression and a tool from AI, neural networks.\n",
    "\n",
    "Before we go too far, logisitic regression is also a tool for stories. As with regression, the narrative often centers around the coefficients in a model, their sign and their significance. The predictions themselves can also be important. Take, for example, the [Surgeon Scorecard](https://projects.propublica.org/surgeons/) from ProPublica (yes, in the application of this kind of tool, ProPublica has a huge advantage). Here, logistic regression is used to separate out a doctor from a hospital effect when considering patient complications after common surgeries (is this a bad doctor in a good hospital? a good doctor in a bad hospital? or some other choice?).\n",
    "\n",
    "Rather than hinge so much on bell curves (as Galton did in deriving regression), logistic regression uses \"binomial\" responses as the model. Think of a coin toss where the probability of turning up heads depends on a set of variables. So the outcome of a surgery depending on a coin toss, the properties of that coin depending on who and where the surgery was performed. Or perhaps, to return to Thursday's data, a county's primary is decided by a coin toss, the properties of that coin depending on demographics, local economics and so on. \n",
    "\n",
    "Let's read in the data and see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = read.csv(\"primary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(winner~region+pres04margin+black06pct+hisp06pct+white06pct+\n",
    "          growth+pct_less_30k+pct_more_100k+pct_hs_grad+pct_homeowner+POP05_SQMI,\n",
    "          data=primary,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**\n",
    "\n",
    "We close with a tool that comes from an entirely different tradition, artificial intelligence. Neural Networks are inspired by the workings of the human nervous system. The research began with a model of how neurons work built from electric circuits. Here's [one beautiful example](http://cs231n.github.io/neural-networks-1/).<br><br>\n",
    "<img src=http://cs231n.github.io/assets/nn1/neuron.png width=300>\n",
    "<br><br>\n",
    "<img src=http://cs231n.github.io/assets/nn1/neuron_model.jpeg width=300>\n",
    "<br>\n",
    "\n",
    "The idea is that neurons \"receive signals from its dendrites and produces output signals along its (single) axon... In the computational model of a neuron, the signals that travel along the axons (e.g. x0) interact multiplicatively (e.g. w0x0) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. w0).\" Mimicing the biological setting, these weights are learnable with experience. \n",
    "\n",
    "This simple neuron can give rise to a variety of architectures, with layers of neurons. Here are two examples. In the first we have one \"hidden\" layer and in the second, two. Each circle in the hidden and output layers are neurons. \n",
    "<br><br>\n",
    "<img src=http://cs231n.github.io/assets/nn1/neural_net.jpeg width=300>\n",
    "<img src=http://cs231n.github.io/assets/nn1/neural_net2.jpeg width=300>\n",
    "<br>\n",
    "The interesting thing is that despite the very different motivations, logistic regression and neural networks share a formalism. A logistic regression is a simple neural network without any hidden layers. This is another case of how story and mathematics influence each other.\n",
    "\n",
    "Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install.packages(\"neuralnet\",repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(neuralnet)\n",
    "library(caTools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a sense of how this works, we'll look at a restricted set of inputs. The program can take a while to fit and unlike logistic regression, the extra flexibility can make the fitting a little fussy. So let's play it safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary = mutate(primary,bwin = ifelse(winner==\"obama\",1,0))\n",
    "primary = select(primary,bwin,black06pct,pct_less_30k,pct_hs_grad)\n",
    "\n",
    "primary = na.omit(primary)\n",
    "dim(primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to split the data into two pieces. One is for \"traning\" the model and one is to \"test\" how well it performs. The idea is that by leaving data out during the fitting process, we can judge how well the model will perform on new data. This strategy works well when you have a lot of data at hand. It is just part of a larger strategy for model fitting that can involve setting aside different sets of data. But this is the simplest for now.\n",
    "\n",
    "The command sample.split() from the \"caTools\" library does this for us. It creates a vector of TRUE and FALSE values, separating our data into training (TRUE) and test (FALSE) via filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = sample.split(1:nrow(primary), SplitRatio = 0.70)\n",
    "\n",
    "train = filter(primary, split)\n",
    "test = filter(primary, !split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(train)\n",
    "dim(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a neural network. We'll start with just one hidden layer with 10 nodes. We use the \"logistic\" activation function (sigmoid) since our lesson started with logistic regression. The rest of the arguments we can talk about but let's just fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = neuralnet(bwin~black06pct+pct_less_30k+pct_hs_grad,\n",
    "               data=train,\n",
    "               hidden=10,\n",
    "               threshold=0.1,\n",
    "               act.fct = \"logistic\",\n",
    "               linear.output = FALSE,\n",
    "               lifesign = \"minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can have a look at the output. We need to use the command compute() rather than predict() because the authors of the neuralnet package chose not to play by the rules. For compute() we pass the fitted model (nn) and then a data set to make predictions from. Here we use the \"test\" data set (but we have to remove the \"bwin\" variable because compute() only wants a data set of the predictor variables in the model. Sigh.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = compute(nn,select(test,-bwin))$net.result\n",
    "\n",
    "table(test$bwin,ifelse(nn_predictions<0.5,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows where the true county winners match and don't. The model didn't do so badly! Let's fit another one, this time with two hidden layers, one with 5 and one with 3 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = neuralnet(bwin~black06pct+pct_less_30k+pct_hs_grad,\n",
    "               data=train,\n",
    "               hidden=c(5,3),\n",
    "               threshold=0.1,\n",
    "               act.fct = \"logistic\",\n",
    "               linear.output = FALSE,\n",
    "               lifesign = \"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = compute(nn,select(test,-bwin))$net.result\n",
    "\n",
    "table(test$bwin,ifelse(nn_predictions<0.5,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More or less the same I believe. \n",
    "\n",
    "If you want, you can compare this to the predictions from the logistic regression fit to the same variables. Points near the 1-1 line mean predictions that are consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = glm(factor(bwin)~.,data=train,family=binomial)\n",
    "glm_predictions = predict(fit,newdata=test,type=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(test$bwin,ifelse(glm_predictions>0.5,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(glm_predictions,nn_predictions)\n",
    "abline(0,1)\n",
    "abline(h=0.5,lty=2)\n",
    "abline(v=0.5,lty=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
